## Техническое видение (MVP, KISS)

Этот документ — отправная точка и технический проект для реализации простого Telegram LLM-ассистента. Мы фиксируем только самое необходимое для быстрой проверки гипотезы.

### 1) Технологии

- Язык и рантайм: Python 3.11 (просто, распространённо, быстрый старт)
- Telegram-библиотека: aiogram
- Режим бота: long polling (без вебхуков)
- LLM‑провайдер: OpenAI‑совместимый API
  - Локальные: ollama, LM Studio, llama.cpp (через OpenAI‑совместимый эндпоинт)
  - Облачные: OpenRouter
  - Подход: единый клиент `openai` с настройкой `base_url` и ключа через конфиг
- Конфигурация: `.env` (python‑dotenv)
- Хранение данных (лиды): CSV файл локально (без БД)
- Логирование: стандартный `logging`, формат простых строк или JSON, уровень INFO
- Уведомления менеджеру: email
- Развёртывание: локально (один процесс), без вебхуков
- Язык бота (MVP): RU

Открытые вопросы по разделу «Технологии»: нет (на MVP все решения зафиксированы).

---

### 2) Принцип разработки
**KISS и фокус на MVP**
- Реализуем только критичные сценарии; всё остальное — после проверки гипотезы.
- Короткие итерации (часы-день), маленькие задачи, быстрые проверки вручную.
- Одна среда на MVP: локально, без dev/stage/prod.
- Код простой и предсказуемый важнее избыточной гибкости.
- Минимум фреймворков и зависимостей: aiogram + стандартная библиотека.
- Без БД, брокеров и очередей на MVP.
- Тестирование прагматичное: ручные end‑to‑end проверки ключевых сценариев + несколько unit‑тестов критичных функций.
- Наблюдаемость через логи и CSV, без сложных систем.
- Безопасность здравого смысла: не хранить лишнее, ключи в .env, не логировать PII.
- Документация краткая: README и этот документ — источники правды.

### 3) Структура проекта
Минимальная структура (MVP):

- `bot/` — код Telegram‑бота (aiogram)
  - `main.py` — точка входа, запуск long polling
  - `handlers.py` — обработчики команд и сообщений
  - `llm_client.py` — клиент OpenAI‑совместимого API (настройки `base_url`, ключ)
  - `prompt.py` — системный промпт и шаблоны сообщений
  - `lead_store.py` — сохранение лидов в CSV (локально)
  - `logging_conf.py` — базовая настройка `logging`
- `config/`
  - `.env.example` — пример переменных окружения
- `doc/` — документация (`product_idea.md`, `vision.md`)
- `README.md`, `LICENSE`
- `requirements.txt`

Переменные окружения (MVP):
- `TELEGRAM_BOT_TOKEN`
- `LLM_BASE_URL`
- `LLM_API_KEY`

### 4) Архитектура проекта
Высокоуровневые компоненты (MVP):

- Пользователь (Telegram) → Telegram Bot API → `bot/main.py` (aiogram, long polling)
- Роутинг и бизнес‑логика диалога → `handlers.py`
- Системный промпт и шаблоны → `prompt.py`
- Клиент LLM (OpenAI‑совместимый) → `llm_client.py` (использует `base_url`, `api_key`)
- Сохранение лидов → `lead_store.py` (CSV локально)
- Логирование → `logging_conf.py` (уровень INFO, консоль/файл)

Принципы:
- Без вебхуков, БД, очередей, фоновых задач.
- Один процесс, линейный поток обработки сообщений.
- Смена LLM‑провайдера через конфиг без изменения остального кода.

### 5) Модель данных
MVP без БД: CSV + состояние в памяти.

Лид (одна строка в CSV):
- `timestamp` (ISO 8601)
- `chat_id` (int)
- `client_name` (str, если сообщил)
- `contact` (str, телефон/Telegram/email — одно поле)
- `intent` (str, кратко: суть запроса)
- `notes` (str, краткое резюме от бота)
- `source` (str, фиксируем `telegram`)
- `status` (str, `new` | `qualified` | `rejected`) — опционально

Диалог (в памяти, per chat_id):
- `stage`: `greeting` | `qualifying` | `offering` | `collecting_contact` | `done`
- `answers`: dict ключевых ответов
- `last_llm_suggestion`: str
- `started_at`: timestamp

### 6) Работа с LLM
Минимальный подход (без ограничителей на MVP):

- Клиент: единый OpenAI‑совместимый клиент (`openai`), параметры из `.env`:
  - `LLM_BASE_URL`, `LLM_API_KEY`, `LLM_MODEL`
- Системный промпт: статичный текст про компанию и услуги, правила тона (вежливо, по делу), границы ответственности, ответы строго на русском.
- Шаблон запроса:
  - system: `{system_prompt}`
  - user: `Контекст: {краткие ответы клиента}\nЗапрос: {последнее сообщение}`
- Контекст диалога: используем краткое суммирование ответов клиента (без жёстких лимитов на количество сообщений в MVP).
- Обработка ошибок: при исключении возвращаем короткий fallback (“Сервис временно недоступен, давайте обменяемся контактами”).
- Локализация: форсируем RU через системный промпт.
- Безопасность: не отправляем лишние PII, не логируем ключи и полные промпты с персональными данными.

### 7) Мониторинг LLM
Минимально: только логи без метрик.

- Логи уровня INFO в консоль/файл.
- Логируем ключевые события: старт/завершение диалога, переходы стадий, ошибки LLM (код/сообщение).
- Не логируем PII, ключи, полный текст промптов.
- Алертов/дашбордов нет на MVP; проверка логов вручную при необходимости.

### 8) Сценарии работы
MVP user flows:

- Новый пользователь:
  1) Приветствие, кратко: чем бот помогает.
  2) Уточнение задачи (2–4 вопроса).
  3) Краткая рекомендация услуги.
  4) Запрос контакта (одно поле).
  5) Благодарность и запись лида в CSV.

- Возвратный пользователь:
  - Если есть незавершённая сессия — продолжить с текущей стадии.
  - Иначе — как «Новый пользователь».

- Сложные/вне компетенции запросы:
  - Вежливый отказ + предложение оставить контакт.

- Отказ оставить контакт:
  - Вежливое завершение, приглашение обратиться позже.

### 9) Деплой
MVP локально (Windows), без вебхуков:

- Среда: Python 3.11, виртуальное окружение (venv).
- Завимости: `requirements.txt`.
- Конфигурация: `.env` с `TELEGRAM_BOT_TOKEN`, `LLM_BASE_URL`, `LLM_API_KEY`.
- Запуск: `python bot/main.py` (long polling).
- Логи: консоль и/или файл через `logging_conf.py`.
- Автозапуск: не требуется на MVP (при необходимости позже добавить планировщик задач).

Шаги:
1) Установить Python 3.11 и создать venv.
2) `pip install -r requirements.txt`.
3) Скопировать `config/.env.example` → `.env` и заполнить значения.
4) Запустить `python bot/main.py`.

### 10) Подход к конфигурированию
MVP через `.env` (локально):

- Формат: `.env`, пример — `config/.env.example` (в репозитории только пример).
- Переменные:
  - `TELEGRAM_BOT_TOKEN`
  - `LLM_BASE_URL`
  - `LLM_API_KEY`
  - `LLM_MODEL`
- Загрузка: через `python-dotenv` при старте (`bot/main.py`).
- Валидация: проверка обязательных переменных, при отсутствии — завершение с сообщением.
- Секреты: хранить только в `.env`, не логировать, не коммитить.

### 11) Подход к логгированию
MVP через стандартный `logging`:

- Уровень: INFO.
- Вывод: консоль; опционально файл `app.log` через базовую настройку в `logging_conf.py`.
- Формат: время, уровень, модуль, сообщение (без персональных данных).
- Логируем:
  - старт/завершение диалога, переходы стадий;
  - ошибки LLM (код/текст исключения, без промптов и PII);
  - успешное сохранение лида (без персональных данных в тексте сообщения).
- Ротация логов: не используется на MVP.


